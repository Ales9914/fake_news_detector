{"cells":[{"cell_type":"markdown","metadata":{},"source":["# NLP for detect fake news using PySpark"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark in c:\\users\\jmgarzonv\\desktop\\eafit\\mmds\\.venv\\lib\\site-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\jmgarzonv\\desktop\\eafit\\mmds\\.venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","\n","os.environ['PYSPARK_PYTHON'] = sys.executable\n","os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession, SQLContext\n","\n","# Initialize Spark session with increased timeout settings\n","ss = SparkSession.builder \\\n","    .appName(\"Fake and Real News\") \\\n","    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n","    .config(\"spark.network.timeout\", \"120s\") \\\n","    .config(\"spark.ui.port\", \"4040\") \\\n","    .config(\"spark.driver.memory\", \"15g\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.executor.cores\", \"2\") \\\n","    .getOrCreate()\n","\n","sc = ss.sparkContext"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Read data set  \n","Reading data using the SparkSession.read.csv method causes structural errors for the data file. So read the data using pandas.read_csv method and convert to Spark Dataframe.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["# Funtion for conver Pandas Dataframe to Spark Dataframe\n","from pyspark.sql.types import StringType, IntegerType, StructField, StructType\n","def read_data(path):\n","  schema= StructType(\n","      [StructField('title',StringType(),True),\n","      StructField('text',StringType(),True),\n","      StructField('label',IntegerType(),True)])\n","  pd_df= pd.read_csv(path).drop('Unnamed: 0', axis= 1)\n","  sp_df= ss.createDataFrame(pd_df, schema= schema)\n","  return sp_df"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["# Read data set\n","path_data= 'WELFake_Dataset.csv'\n","data= read_data(path_data)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Check the data set"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+-----+\n","|                text|label|\n","+--------------------+-----+\n","|No comment is exp...|    1|\n","|Did they post the...|    1|\n","| Now, most of the...|    1|\n","|A dozen political...|    0|\n","|The RS-28 Sarmat ...|    1|\n","+--------------------+-----+\n","only showing top 5 rows\n","\n"]}],"source":["data[\"text\", \"label\"].show(5)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Create objects for processing data"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from pyspark.ml import Transformer\n","from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n","from pyspark.ml.linalg import Vectors, VectorUDT\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import StructType, StructField, DoubleType, ArrayType\n","import numpy as np\n","\n","class TfidfFilter(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n","    def __init__(self, inputCol=None, outputCol=None, threshold=0.1):\n","        super(TfidfFilter, self).__init__()\n","        self.inputCol = inputCol\n","        self.outputCol = outputCol\n","        self.threshold = threshold\n","    \n","    def _transform(self, dataset):\n","        def filter_tfidf(vec):\n","            indices = [i for i, v in enumerate(vec.toArray()) if v > self.threshold]\n","            values = [v for v in vec.toArray() if v > self.threshold]\n","            return Vectors.sparse(vec.size, indices, values)\n","        \n","        filter_udf = udf(filter_tfidf, VectorUDT())\n","        return dataset.withColumn(self.outputCol, filter_udf(self.inputCol))"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["from pyspark.ml.feature import SQLTransformer, RegexTokenizer, StopWordsRemover, CountVectorizer, Imputer, IDF\n","from pyspark.ml.feature import StringIndexer, VectorAssembler\n","StopWordsRemover.loadDefaultStopWords('english')\n","\n","# 0. Extract tokens from title\n","title_tokenizer= RegexTokenizer(inputCol= 'title', outputCol= 'title_words',\n","                                pattern= '\\\\W', toLowercase= True)\n","# 1. Remove stop words from title\n","title_sw_remover= StopWordsRemover(inputCol= 'title_words', outputCol= 'title_sw_removed')\n","# 2. Compute Term frequency from title\n","title_count_vectorizer= CountVectorizer(inputCol= 'title_sw_removed', outputCol= 'tf_title')\n","# 3. Compute Term frequency-inverse document frequency from title\n","title_tfidf= IDF(inputCol= 'tf_title', outputCol= 'tf_idf_title')\n","# 4. Extract tokens from text\n","text_tokenizer= RegexTokenizer(inputCol= 'text', outputCol= 'text_words',\n","                                pattern= '\\\\W', toLowercase= True)\n","# 5. Remove stop words from text\n","text_sw_remover= StopWordsRemover(inputCol= 'text_words', outputCol= 'text_sw_removed')\n","# 6. Compute Term frequency from text\n","text_count_vectorizer= CountVectorizer(inputCol= 'text_sw_removed', outputCol= 'tf_text')\n","# 7. Compute Term frequency-inverse document frequency text\n","text_tfidf= IDF(inputCol= 'tf_text', outputCol= 'tf_idf_text')\n","\n","# Create the custom TF-IDF filter transformer\n","tfidf_filter = TfidfFilter(inputCol='tf_idf_text', outputCol='filtered_tf_idf_text', threshold=0.15)\n","\n","# 9. VectorAssembler\n","vec_assembler= VectorAssembler(inputCols=['tf_idf_title', 'filtered_tf_idf_text'], outputCol= 'features')"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Create object for Random Forest Classifier model"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["from pyspark.ml.classification import RandomForestClassifier\n","# 10 Random Forest Classifier\n","rf= RandomForestClassifier(featuresCol= 'features', labelCol= 'label', predictionCol= 'label_predict', maxDepth= 7, numTrees= 20)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1. Create Pipeline for processing and fitting data to model"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["from pyspark.ml import Pipeline\n","rf_pipe= Pipeline(stages=[title_tokenizer, # 0\n","                title_sw_remover, # 1\n","                title_count_vectorizer, # 2\n","                title_tfidf, # 3\n","                text_tokenizer, # 4\n","                text_sw_remover, # 5\n","                text_count_vectorizer, # 6\n","                text_tfidf, # 7\n","                tfidf_filter,         # 8\n","                vec_assembler,        # 9\n","                rf]) # 10 model"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Create object for Logistic Regression model"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from pyspark.ml.classification import LogisticRegression\n","\n","# Define the Logistic Regression model\n","lr = LogisticRegression(featuresCol='features', labelCol='label', predictionCol='label_predict', maxIter=10)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.1 Create Pipeline for processing and fitting data to model (LR)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Create the pipeline\n","lr_pipe = Pipeline(stages=[\n","    title_tokenizer,      # 0\n","    title_sw_remover,     # 1\n","    title_count_vectorizer, # 2\n","    title_tfidf,          # 3\n","    text_tokenizer,       # 4\n","    text_sw_remover,      # 5\n","    text_count_vectorizer, # 6\n","    text_tfidf,           # 7\n","    tfidf_filter,         # 8\n","    vec_assembler,        # 9\n","    lr                    # 10 model\n","])"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Splitting the dataset into the training set and test set"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["train, test= data.randomSplit([0.8, 0.2])"]},{"cell_type":"markdown","metadata":{},"source":["## 7. Fitting the models"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["rf_model= rf_pipe.fit(train)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["lr_model = lr_pipe.fit(train)"]},{"cell_type":"markdown","metadata":{},"source":["## 8. Evaluate classification model"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["# Function for evaluating classification model\n","from pyspark.ml.evaluation import  MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","\n","accuracy= MulticlassClassificationEvaluator(labelCol= 'label', predictionCol= 'label_predict', metricName= 'accuracy')\n","f1= MulticlassClassificationEvaluator(labelCol= 'label', predictionCol= 'label_predict', metricName= 'f1')\n","areaUnderROC= BinaryClassificationEvaluator(labelCol= 'label', metricName= 'areaUnderROC')\n","\n","def classification_evaluator(data_result):\n","    data_result.crosstab(col1= 'label_predict', col2= 'label').show()\n","    print('accuracy:' ,accuracy.evaluate(data_result))\n","    print('f1:' ,f1.evaluate(data_result))\n","    print('areaUnderROC:' ,areaUnderROC.evaluate(data_result))"]},{"cell_type":"markdown","metadata":{},"source":["### 8.1 Evaluation of final model fit on the training data set"]},{"cell_type":"markdown","metadata":{},"source":["#### Random Forest"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["# Predict on training data set\n","rf_train_result= rf_model.transform(train)"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+-----+-----+\n","|label_predict_label|    0|    1|\n","+-------------------+-----+-----+\n","|                1.0| 8675|26772|\n","|                0.0|19353| 2883|\n","+-------------------+-----+-----+\n","\n","accuracy: 0.7996290068130992\n","f1: 0.796997785382555\n","areaUnderROC: 0.9013619254026799\n"]}],"source":["classification_evaluator(rf_train_result)"]},{"cell_type":"markdown","metadata":{},"source":["#### Logistic Regression"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["lr_train_result = lr_model.transform(train)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+-----+-----+\n","|label_predict_label|    0|    1|\n","+-------------------+-----+-----+\n","|                1.0|    7|29655|\n","|                0.0|28021|    0|\n","+-------------------+-----+-----+\n","\n","accuracy: 0.9998786470883969\n","f1: 0.9998786466709\n","areaUnderROC: 0.999967578847917\n"]}],"source":["classification_evaluator(lr_train_result)"]},{"cell_type":"markdown","metadata":{},"source":["### 8.2 evaluation of final model fit on the test data set"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["# Predict on test data set\n","rf_test_result= rf_model.transform(test)"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+----+----+\n","|label_predict_label|   0|   1|\n","+-------------------+----+----+\n","|                1.0|2182|6658|\n","|                0.0|4818| 793|\n","+-------------------+----+----+\n","\n","accuracy: 0.7941318939865754\n","f1: 0.7915708804829462\n","areaUnderROC: 0.8955413271468835\n"]}],"source":["classification_evaluator(rf_test_result)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["lr_test_result = lr_model.transform(test)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+----+----+\n","|label_predict_label|   0|   1|\n","+-------------------+----+----+\n","|                1.0| 245|7218|\n","|                0.0|6755| 233|\n","+-------------------+----+----+\n","\n","accuracy: 0.9669227043111204\n","f1: 0.9669218233789496\n","areaUnderROC: 0.9908905228444894\n"]}],"source":["classification_evaluator(lr_test_result)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
